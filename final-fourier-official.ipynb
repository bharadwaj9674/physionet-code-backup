{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1d58ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bharath/miniforge3/lib/python3.10/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import os, numpy as np, scipy as sp, scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99182808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Do *not* edit this script.\n",
    "# These are helper functions that you can use with your code.\n",
    "# Check the example code to see how to import these functions to your code.\n",
    "\n",
    "### Challenge data I/O functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3dbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the folders with data files.\n",
    "def find_data_folders(root_folder):\n",
    "    data_folders = list()\n",
    "    for x in sorted(os.listdir(root_folder)):\n",
    "        data_folder = os.path.join(root_folder, x)\n",
    "        if os.path.isdir(data_folder):\n",
    "            data_file = os.path.join(data_folder, x + '.txt')\n",
    "            if os.path.isfile(data_file):\n",
    "                data_folders.append(x)\n",
    "    return sorted(data_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ab072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the patient metadata: age, sex, etc.\n",
    "def load_challenge_data(data_folder, patient_id):\n",
    "    patient_metadata_file = os.path.join(data_folder, patient_id, patient_id + '.txt')\n",
    "    patient_metadata = load_text_file(patient_metadata_file)\n",
    "    return patient_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229928d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the record names.\n",
    "def find_recording_files(data_folder, patient_id):\n",
    "    record_names = list()\n",
    "    patient_folder = os.path.join(data_folder, patient_id)\n",
    "    for file_name in sorted(os.listdir(patient_folder)):\n",
    "        if not file_name.startswith('.') and file_name.endswith('.hea'):\n",
    "            root, ext = os.path.splitext(file_name)\n",
    "            record_name = '_'.join(root.split('_')[:-1])\n",
    "            record_names.append(record_name)\n",
    "    return sorted(record_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3f9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WFDB data for the Challenge (but not all possible WFDB files).\n",
    "def load_recording_data(record_name):\n",
    "    # Allow either the record name or the header filename.\n",
    "    root, ext = os.path.splitext(record_name)\n",
    "    if ext=='':\n",
    "        header_file = record_name + '.hea'\n",
    "    else:\n",
    "        header_file = record_name\n",
    "\n",
    "    # Load the header file.\n",
    "    if not os.path.isfile(header_file):\n",
    "        raise FileNotFoundError('{} recording not found.'.format(record_name))\n",
    "\n",
    "    with open(header_file, 'r') as f:\n",
    "        header = [l.strip() for l in f.readlines() if l.strip()]\n",
    "\n",
    "    # Parse the header file.\n",
    "    record_name = None\n",
    "    num_signals = None\n",
    "    sampling_frequency = None\n",
    "    num_samples = None\n",
    "    signal_files = list()\n",
    "    gains = list()\n",
    "    offsets = list()\n",
    "    channels = list()\n",
    "    initial_values = list()\n",
    "    checksums = list()\n",
    "\n",
    "    for i, l in enumerate(header):\n",
    "        arrs = [arr.strip() for arr in l.split(' ')]\n",
    "        # Parse the record line.\n",
    "        if i==0:\n",
    "            record_name = arrs[0]\n",
    "            num_signals = int(arrs[1])\n",
    "            sampling_frequency = float(arrs[2])\n",
    "            num_samples = int(arrs[3])\n",
    "        # Parse the signal specification lines.\n",
    "        elif not l.startswith('#') or len(l.strip()) == 0:\n",
    "            signal_file = arrs[0]\n",
    "            gain = float(arrs[2].split('/')[0])\n",
    "            offset = int(arrs[4])\n",
    "            initial_value = int(arrs[5])\n",
    "            checksum = int(arrs[6])\n",
    "            channel = arrs[8]\n",
    "            signal_files.append(signal_file)\n",
    "            gains.append(gain)\n",
    "            offsets.append(offset)\n",
    "            initial_values.append(initial_value)\n",
    "            checksums.append(checksum)\n",
    "            channels.append(channel)\n",
    "\n",
    "    # Check that the header file only references one signal file. WFDB format allows for multiple signal files, but, for\n",
    "    # simplicity, we have not done that here.\n",
    "    num_signal_files = len(set(signal_files))\n",
    "    if num_signal_files!=1:\n",
    "        raise NotImplementedError('The header file {}'.format(header_file) \\\n",
    "            + ' references {} signal files; one signal file expected.'.format(num_signal_files))\n",
    "\n",
    "    # Load the signal file.\n",
    "    head, tail = os.path.split(header_file)\n",
    "    signal_file = os.path.join(head, list(signal_files)[0])\n",
    "    data = np.asarray(sp.io.loadmat(signal_file)['val'])\n",
    "\n",
    "    # Check that the dimensions of the signal data in the signal file is consistent with the dimensions for the signal data given\n",
    "    # in the header file.\n",
    "    num_channels = len(channels)\n",
    "    if np.shape(data)!=(num_channels, num_samples):\n",
    "        raise ValueError('The header file {}'.format(header_file) \\\n",
    "            + ' is inconsistent with the dimensions of the signal file.')\n",
    "\n",
    "    # Check that the initial value and checksums for the signal data in the signal file are consistent with the initial value and\n",
    "    # checksums for the signal data given in the header file.\n",
    "    for i in range(num_channels):\n",
    "        if data[i, 0]!=initial_values[i]:\n",
    "            raise ValueError('The initial value in header file {}'.format(header_file) \\\n",
    "                + ' is inconsistent with the initial value for channel'.format(channels[i]))\n",
    "        if np.sum(data[i, :])!=checksums[i]:\n",
    "            raise ValueError('The checksum in header file {}'.format(header_file) \\\n",
    "                + ' is inconsistent with the initial value for channel'.format(channels[i]))\n",
    "\n",
    "    # Rescale the signal data using the gains and offsets.\n",
    "    rescaled_data = np.zeros(np.shape(data), dtype=np.float32)\n",
    "    for i in range(num_channels):\n",
    "        rescaled_data[i, :] = (data[i, :]-offsets[i])/gains[i]\n",
    "\n",
    "    return rescaled_data, channels, sampling_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a12ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the channels.\n",
    "def reduce_channels(current_data, current_channels, requested_channels):\n",
    "    if current_channels == requested_channels:\n",
    "        reduced_data = current_data\n",
    "        reduced_channels = current_channels\n",
    "    else:\n",
    "        reduced_indices = [current_channels.index(channel) for channel in requested_channels if channel in current_channels]\n",
    "        reduced_channels = [current_channels[i] for i in reduced_indices]\n",
    "        reduced_data = current_data[reduced_indices, :]\n",
    "    return reduced_data, reduced_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e0dde43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the channels.\n",
    "def expand_channels(current_data, current_channels, requested_channels):\n",
    "    if current_channels == requested_channels:\n",
    "        expanded_data = current_data\n",
    "    else:\n",
    "        num_current_channels, num_samples = np.shape(current_data)\n",
    "        num_requested_channels = len(requested_channels)\n",
    "        expanded_data = np.zeros((num_requested_channels, num_samples))\n",
    "        for i, channel in enumerate(requested_channels):\n",
    "            if channel in current_channels:\n",
    "                j = current_channels.index(channel)\n",
    "                expanded_data[i, :] = current_data[j, :]\n",
    "            else:\n",
    "                expanded_data[i, :] = float('nan')\n",
    "    return expanded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c253ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Challenge data I/O functions\n",
    "\n",
    "# Load text file as a string.\n",
    "def load_text_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "# Get a variable from the patient metadata.\n",
    "def get_variable(text, variable_name, variable_type):\n",
    "    variable = None\n",
    "    for l in text.split('\\n'):\n",
    "        if l.startswith(variable_name):\n",
    "            variable = ':'.join(l.split(':')[1:]).strip()\n",
    "            variable = cast_variable(variable, variable_type)\n",
    "            return variable\n",
    "\n",
    "# Get the patient ID variable from the patient data.\n",
    "def get_patient_id(string):\n",
    "    return get_variable(string, 'Patient', str)\n",
    "\n",
    "# Get the patient ID variable from the patient data.\n",
    "def get_hospital(string):\n",
    "    return get_variable(string, 'Hospital', str)\n",
    "\n",
    "# Get the age variable (in years) from the patient data.\n",
    "def get_age(string):\n",
    "    return get_variable(string, 'Age', int)\n",
    "\n",
    "# Get the sex variable from the patient data.\n",
    "def get_sex(string):\n",
    "    return get_variable(string, 'Sex', str)\n",
    "\n",
    "# Get the ROSC variable (in minutes) from the patient data.\n",
    "def get_rosc(string):\n",
    "    return get_variable(string, 'ROSC', int)\n",
    "\n",
    "# Get the OHCA variable from the patient data.\n",
    "def get_ohca(string):\n",
    "    return get_variable(string, 'OHCA', bool)\n",
    "\n",
    "# Get the shockable rhythm variable from the patient data.\n",
    "def get_shockable_rhythm(string):\n",
    "    return get_variable(string, 'Shockable Rhythm', bool)\n",
    "\n",
    "# Get the TTM variable (in Celsius) from the patient data.\n",
    "def get_ttm(string):\n",
    "    return get_variable(string, 'TTM', int)\n",
    "\n",
    "# Get the Outcome variable from the patient data.\n",
    "def get_outcome(string):\n",
    "    variable = get_variable(string, 'Outcome', str)\n",
    "    if variable is None or is_nan(variable):\n",
    "        raise ValueError('No outcome available. Is your code trying to load labels from the hidden data?')\n",
    "    if variable == 'Good':\n",
    "        variable = 0\n",
    "    elif variable == 'Poor':\n",
    "        variable = 1\n",
    "    return variable\n",
    "\n",
    "# Get the Outcome probability variable from the patient data.\n",
    "def get_outcome_probability(string):\n",
    "    variable = sanitize_scalar_value(get_variable(string, 'Outcome Probability', str))\n",
    "    if variable is None or is_nan(variable):\n",
    "        raise ValueError('No outcome available. Is your code trying to load labels from the hidden data?')\n",
    "    return variable\n",
    "\n",
    "# Get the CPC variable from the patient data.\n",
    "def get_cpc(string):\n",
    "    variable = sanitize_scalar_value(get_variable(string, 'CPC', str))\n",
    "    if variable is None or is_nan(variable):\n",
    "        raise ValueError('No CPC score available. Is your code trying to load labels from the hidden data?')\n",
    "    return variable\n",
    "\n",
    "# Get the utility frequency (in Hertz) from the recording data.\n",
    "def get_utility_frequency(string):\n",
    "    return get_variable(string, '#Utility frequency', int)\n",
    "\n",
    "# Get the start time (in hh:mm:ss format) from the recording data.\n",
    "def get_start_time(string):\n",
    "    variable = get_variable(string, '#Start time', str)\n",
    "    times = tuple(int(value) for value in variable.split(':'))\n",
    "    return times\n",
    "\n",
    "# Get the end time (in hh:mm:ss format) from the recording data.\n",
    "def get_end_time(string):\n",
    "    variable = get_variable(string, '#End time', str)\n",
    "    times = tuple(int(value) for value in variable.split(':'))\n",
    "    return times\n",
    "\n",
    "# Convert seconds to days, hours, minutes, seconds.\n",
    "def convert_seconds_to_hours_minutes_seconds(seconds):\n",
    "    hours = int(seconds/3600 - 24*days)\n",
    "    minutes = int(seconds/60 - 24*60*days - 60*hours)\n",
    "    seconds = int(seconds - 24*3600*days - 3600*hours - 60*minutes)\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "# Convert hours, minutes, and seconds to seconds.\n",
    "def convert_hours_minutes_seconds_to_seconds(hours, minutes, seconds):\n",
    "    return 3600*hours + 60*minutes + seconds\n",
    "\n",
    "### Challenge label and output I/O functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e50b3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Challenge outputs for one file.\n",
    "def save_challenge_outputs(filename, patient_id, outcome, outcome_probability, cpc):\n",
    "    # Sanitize values, e.g., in case they are a singleton array.\n",
    "    outcome = sanitize_boolean_value(outcome)\n",
    "    outcome_probability = sanitize_scalar_value(outcome_probability)\n",
    "    cpc = sanitize_scalar_value(cpc)\n",
    "\n",
    "    # Format Challenge outputs.\n",
    "    patient_string = 'Patient: {}'.format(patient_id)\n",
    "    if outcome == 0:\n",
    "        outcome = 'Good'\n",
    "    elif outcome == 1:\n",
    "        outcome = 'Poor'\n",
    "    outcome_string = 'Outcome: {}'.format(outcome)\n",
    "    outcome_probability_string = 'Outcome Probability: {:.3f}'.format(outcome_probability)\n",
    "    cpc_string = 'CPC: {:.3f}'.format(cast_int_if_int_else_float(cpc))\n",
    "    output_string = patient_string + '\\n' + \\\n",
    "        outcome_string + '\\n' + outcome_probability_string + '\\n' + cpc_string + '\\n'\n",
    "\n",
    "    # Write the Challenge outputs.\n",
    "    if filename is not None:\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(output_string)\n",
    "\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ed7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Other helper functions\n",
    "\n",
    "# Check if a variable is a number or represents a number.\n",
    "def is_number(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "# Check if a variable is an integer or represents an integer.\n",
    "def is_integer(x):\n",
    "    if is_number(x):\n",
    "        return float(x).is_integer()\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Check if a variable is a boolean or represents a boolean.\n",
    "def is_boolean(x):\n",
    "    if (is_number(x) and float(x)==0) or (remove_extra_characters(x) in ('False', 'false', 'FALSE', 'F', 'f')):\n",
    "        return True\n",
    "    elif (is_number(x) and float(x)==1) or (remove_extra_characters(x) in ('True', 'true', 'TRUE', 'T', 't')):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Check if a variable is a finite number or represents a finite number.\n",
    "def is_finite_number(x):\n",
    "    if is_number(x):\n",
    "        return np.isfinite(float(x))\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Check if a variable is a NaN (not a number) or represents a NaN.\n",
    "def is_nan(x):\n",
    "    if is_number(x):\n",
    "        return np.isnan(float(x))\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Remove any quotes, brackets (for singleton arrays), and/or invisible characters.\n",
    "def remove_extra_characters(x):\n",
    "    return str(x).replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '').replace(' ', '').strip()\n",
    "\n",
    "# Sanitize boolean values.\n",
    "def sanitize_boolean_value(x):\n",
    "    x = remove_extra_characters(x)\n",
    "    if (is_number(x) and float(x)==0) or (remove_extra_characters(x) in ('False', 'false', 'FALSE', 'F', 'f')):\n",
    "        return 0\n",
    "    elif (is_number(x) and float(x)==1) or (remove_extra_characters(x) in ('True', 'true', 'TRUE', 'T', 't')):\n",
    "        return 1\n",
    "    else:\n",
    "        return float('nan')\n",
    "\n",
    "# Sanitize integer values.\n",
    "def sanitize_integer_value(x):\n",
    "    x = remove_extra_characters(x)\n",
    "    if is_integer(x):\n",
    "        return int(float(x))\n",
    "    else:\n",
    "        return float('nan')\n",
    "\n",
    "# Sanitize scalar values.\n",
    "def sanitize_scalar_value(x):\n",
    "    x = remove_extra_characters(x)\n",
    "    if is_number(x):\n",
    "        return float(x)\n",
    "    else:\n",
    "        return float('nan')\n",
    "\n",
    "# Cast a value to a particular type.\n",
    "def cast_variable(variable, variable_type, preserve_nan=True):\n",
    "    if preserve_nan and is_nan(variable):\n",
    "        variable = float('nan')\n",
    "    else:\n",
    "        if variable_type == bool:\n",
    "            variable = sanitize_boolean_value(variable)\n",
    "        elif variable_type == int:\n",
    "            variable = sanitize_integer_value(variable)\n",
    "        elif variable_type == float:\n",
    "            variable = sanitize_scalar_value(variable)\n",
    "        else:\n",
    "            variable = variable_type(variable)\n",
    "    return variable\n",
    "\n",
    "# Cast a value to an integer if the value is an integer, a float if the value is a non-integer float, and itself otherwise.\n",
    "def cast_int_if_int_else_float(x):\n",
    "    if is_integer(x):\n",
    "        return int(float(x))\n",
    "    elif is_number(x):\n",
    "        return float(x)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6771c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Load libraries.\n",
    "import os, sys, shutil, argparse\n",
    "\n",
    "# Parse arguments.\n",
    "def get_parser():\n",
    "    description = 'Remove data from the dataset.'\n",
    "    parser = argparse.ArgumentParser(description=description)\n",
    "    parser.add_argument('-i', '--input_folder', type=str, required=True)\n",
    "    parser.add_argument('-p', '--patient_ids', nargs='*', type=str, required=False, default=[])\n",
    "    parser.add_argument('-o', '--output_folder', type=str, required=True)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "259a0263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find folders with data files.\n",
    "def find_data_folders(root_folder):\n",
    "    data_folders = list()\n",
    "    for x in sorted(os.listdir(root_folder)):\n",
    "        data_folder = os.path.join(root_folder, x)\n",
    "        if os.path.isdir(data_folder):\n",
    "            data_file = os.path.join(data_folder, x + '.txt')\n",
    "            if os.path.isfile(data_file):\n",
    "                data_folders.append(x)\n",
    "    return sorted(data_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df79b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script.\n",
    "def run(args):\n",
    "    # Use either the given patient IDs or all of the patient IDs.\n",
    "    if args.patient_ids:\n",
    "        patient_ids = args.patient_ids\n",
    "    else:\n",
    "        patient_ids = find_data_folders(args.input_folder)\n",
    "\n",
    "    # Iterate over the patient IDs.\n",
    "    for patient_id in patient_ids:\n",
    "        input_path = os.path.join(args.input_folder, patient_id)\n",
    "        output_path = os.path.join(args.output_folder, patient_id)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Iterate over the files in each folder.\n",
    "        for file_name in sorted(os.listdir(input_path)):\n",
    "            file_root, file_ext = os.path.splitext(file_name)\n",
    "            input_file = os.path.join(input_path, file_name)\n",
    "            output_file = os.path.join(output_path, file_name)\n",
    "\n",
    "            # If the file is not the binary signal data, then copy it.\n",
    "            if not (file_ext == '.mat'):\n",
    "                shutil.copy2(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b8b61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Load libraries.\n",
    "import os, sys, shutil, argparse\n",
    "\n",
    "# Parse arguments.\n",
    "def get_parser():\n",
    "    description = 'Remove labels from the dataset.'\n",
    "    parser = argparse.ArgumentParser(description=description)\n",
    "    parser.add_argument('-i', '--input_folder', type=str, required=True)\n",
    "    parser.add_argument('-p', '--patient_ids', nargs='*', type=str, required=False, default=[])\n",
    "    parser.add_argument('-o', '--output_folder', type=str, required=True)\n",
    "    return parser\n",
    "\n",
    "# Find folders with data files.\n",
    "def find_data_folders(root_folder):\n",
    "    data_folders = list()\n",
    "    for x in sorted(os.listdir(root_folder)):\n",
    "        data_folder = os.path.join(root_folder, x)\n",
    "        if os.path.isdir(data_folder):\n",
    "            data_file = os.path.join(data_folder, x + '.txt')\n",
    "            if os.path.isfile(data_file):\n",
    "                data_folders.append(x)\n",
    "    return sorted(data_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16085d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script.\n",
    "def run(args):\n",
    "    # Use either the given patient IDs or all of the patient IDs.\n",
    "    if args.patient_ids:\n",
    "        patient_ids = args.patient_ids\n",
    "    else:\n",
    "        patient_ids = find_data_folders(args.input_folder)\n",
    "\n",
    "    # Iterate over the patient IDs.\n",
    "    for patient_id in patient_ids:\n",
    "        input_path = os.path.join(args.input_folder, patient_id)\n",
    "        output_path = os.path.join(args.output_folder, patient_id)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Iterate over the files in each folder.\n",
    "        for file_name in sorted(os.listdir(input_path)):\n",
    "            file_root, file_ext = os.path.splitext(file_name)\n",
    "            input_file = os.path.join(input_path, file_name)\n",
    "            output_file = os.path.join(output_path, file_name)\n",
    "\n",
    "            # If the file does have the labels, then remove the labels and copy the rest of the file.\n",
    "            if file_ext == '.txt' and file_root == patient_id:\n",
    "                with open(input_file, 'r') as f:\n",
    "                    input_lines = f.readlines()\n",
    "                output_lines = [l for l in input_lines if not (l.startswith('Outcome') or l.startswith('CPC'))]\n",
    "                output_string = ''.join(output_lines)\n",
    "                with open(output_file, 'w') as f:\n",
    "                    f.write(output_string)\n",
    "\n",
    "            # Otherwise, copy the file as-is.\n",
    "            else:\n",
    "                shutil.copy2(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dbce77",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "969c89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your trained model.\n",
    "def save_challenge_model(model_folder, imputer, outcome_model, cpc_model):\n",
    "    d = {'imputer': imputer, 'outcome_model': outcome_model, 'cpc_model': cpc_model}\n",
    "    filename = os.path.join(model_folder, 'models.sav')\n",
    "    joblib.dump(d, filename, protocol=0)\n",
    "\n",
    "# Preprocess data.\n",
    "def preprocess_data(data, sampling_frequency, utility_frequency):\n",
    "    # Define the bandpass frequencies.\n",
    "    passband = [0.1, 30.0]\n",
    "\n",
    "    # Promote the data to double precision because these libraries expect double precision.\n",
    "    data = np.asarray(data, dtype=np.float64)\n",
    "\n",
    "    # If the utility frequency is between bandpass frequencies, then apply a notch filter.\n",
    "    if utility_frequency is not None and passband[0] <= utility_frequency <= passband[1]:\n",
    "        data = mne.filter.notch_filter(data, sampling_frequency, utility_frequency, n_jobs=4, verbose='error')\n",
    "\n",
    "    # Apply a bandpass filter.\n",
    "    data = mne.filter.filter_data(data, sampling_frequency, passband[0], passband[1], n_jobs=4, verbose='error')\n",
    "\n",
    "    # Resample the data.\n",
    "    if sampling_frequency % 2 == 0:\n",
    "        resampling_frequency = 128\n",
    "    else:\n",
    "        resampling_frequency = 125\n",
    "    lcm = np.lcm(int(round(sampling_frequency)), int(round(resampling_frequency)))\n",
    "    up = int(round(lcm / sampling_frequency))\n",
    "    down = int(round(lcm / resampling_frequency))\n",
    "    resampling_frequency = sampling_frequency * up / down\n",
    "    data = scipy.signal.resample_poly(data, up, down, axis=1)\n",
    "    \n",
    "    print\n",
    "\n",
    "    # Scale the data to the interval [-1, 1].\n",
    "    min_value = np.min(data)\n",
    "    max_value = np.max(data)\n",
    "    if min_value != max_value:\n",
    "        data = 2.0 / (max_value - min_value) * (data - 0.5 * (min_value + max_value))\n",
    "    else:\n",
    "        data = 0 * data\n",
    "\n",
    "    return data, resampling_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a29ea587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features.\n",
    "def get_features(data_folder, patient_id):\n",
    "    # Load patient data.\n",
    "    patient_metadata = load_challenge_data(data_folder, patient_id)\n",
    "    recording_ids = find_recording_files(data_folder, patient_id)\n",
    "    num_recordings = len(recording_ids)\n",
    "\n",
    "    # Extract patient features.\n",
    "    patient_features = get_patient_features(patient_metadata)\n",
    "\n",
    "    # Extract EEG features.\n",
    "#     eeg_channels = ['F3', 'P3', 'F4', 'P4']\n",
    "    group = 'EEG'\n",
    "\n",
    "#     if num_recordings > 0:\n",
    "#         recording_id = recording_ids[-1]\n",
    "#         recording_location = os.path.join(data_folder, patient_id, '{}_{}'.format(recording_id, group))\n",
    "#         if os.path.exists(recording_location + '.hea'):\n",
    "#             data, channels, sampling_frequency = load_recording_data(recording_location)\n",
    "#             utility_frequency = get_utility_frequency(recording_location + '.hea')\n",
    "\n",
    "#             if all(channel in channels for channel in eeg_channels):\n",
    "#                 data, channels = reduce_channels(data, channels, eeg_channels)\n",
    "#                 data, sampling_frequency = preprocess_data(data, sampling_frequency, utility_frequency)\n",
    "#                 data = np.array([data[0, :] - data[1, :], data[2, :] - data[3, :]]) # Convert to bipolar montage: F3-P3 and F4-P4\n",
    "#                 eeg_features = get_eeg_features(data, sampling_frequency).flatten()\n",
    "#             else:\n",
    "#                 eeg_features = float('nan') * np.ones(8) # 2 bipolar channels * 4 features / channel\n",
    "#         else:\n",
    "#             eeg_features = float('nan') * np.ones(8) # 2 bipolar channels * 4 features / channel\n",
    "#     else:\n",
    "#         eeg_features = float('nan') * np.ones(8) # 2 bipolar channels * 4 features / channel\n",
    "\n",
    "#     # Extract ECG features.\n",
    "#     ecg_channels = ['ECG', 'ECGL', 'ECGR', 'ECG1', 'ECG2']\n",
    "#     group = 'ECG'\n",
    "\n",
    "#     if num_recordings > 0:\n",
    "#         recording_id = recording_ids[0]\n",
    "#         recording_location = os.path.join(data_folder, patient_id, '{}_{}'.format(recording_id, group))\n",
    "#         if os.path.exists(recording_location + '.hea'):\n",
    "#             data, channels, sampling_frequency = load_recording_data(recording_location)\n",
    "#             utility_frequency = get_utility_frequency(recording_location + '.hea')\n",
    "\n",
    "#             data, channels = reduce_channels(data, channels, ecg_channels)\n",
    "#             data, sampling_frequency = preprocess_data(data, sampling_frequency, utility_frequency)\n",
    "#             features = get_ecg_features(data)\n",
    "#             ecg_features = expand_channels(features, channels, ecg_channels).flatten()\n",
    "#         else:\n",
    "#             ecg_features = float('nan') * np.ones(10) # 5 channels * 2 features / channel\n",
    "#     else:\n",
    "#         ecg_features = float('nan') * np.ones(10) # 5 channels * 2 features / channel\n",
    "        \n",
    "    group = 'EEG'\n",
    "    if num_recordings > 0:\n",
    "        recording_id = recording_ids[-1]\n",
    "        recording_location = os.path.join(data_folder, patient_id, '{}_{}'.format(recording_id, group))\n",
    "        if os.path.exists(recording_location + '.hea'):\n",
    "            data, channels, sampling_frequency = load_recording_data(recording_location)\n",
    "            utility_frequency = get_utility_frequency(recording_location + '.hea')\n",
    "            eeg_features_fourier = get_fourier_features(data).flatten()\n",
    "            \n",
    "        else:\n",
    "            eeg_features_fourier = float('nan') * np.ones(38)#ipolar channels * 4 features / channel \n",
    "    else:\n",
    "        eeg_features_fourier = float('nan') * np.ones(38)# 2bipolar channels * 4 features / channel\n",
    "        \n",
    "           \n",
    "    ## trying to get all features\n",
    "        \n",
    "    if num_recordings > 0:\n",
    "        \n",
    "        for recording_id in recording_ids:\n",
    "            group = 'EEG'\n",
    "            recording_location = os.path.join(data_folder, patient_id, '{}_{}'.format(recording_id, group))\n",
    "            encoding='utf-8'\n",
    "                \n",
    "            eeg_channels = ['Fp1', 'F7', 'T3', 'T5', 'O1', 'Fp2', 'F8', 'T4', 'T6', 'O2', \n",
    "                            'F3', 'C3', 'P3', 'F4', 'C4', 'P4', 'Fz', 'Cz', 'Pz']\n",
    "            \n",
    "            \n",
    "            recording_location = os.path.join(data_folder, patient_id, '{}_{}'.format(recording_id, group))\n",
    "            if os.path.exists(recording_location + '.hea'):\n",
    "                data, channels, sampling_frequency = load_recording_data(recording_location)\n",
    "                if all(channel in channels for channel in eeg_channels):\n",
    "                    data, channels = reduce_channels(data, channels, eeg_channels)\n",
    "                \n",
    "                \n",
    "            if os.path.exists(recording_location + '.hea'):\n",
    "                file_hea = recording_location + '.hea'\n",
    "                with open(file_hea, 'r', encoding=encoding, errors='replace') as file:\n",
    "                        file_content = file.read()\n",
    "                first_line_tokens = file_content.strip().split()\n",
    "                sampling_frequency = int(first_line_tokens[2])\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        # resample signal data to 100hz\n",
    "        \n",
    "        print(len(recording_ids))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # horizontal stack available recording and fill as zero for the rest of the recordings\n",
    "        \n",
    "        \n",
    "        recording_location = os.path.join(data_folder, patient_id, '{}_{}'.format(recording_id, group))\n",
    "        if os.path.exists(recording_location + '.hea'):\n",
    "            print(recording_location)\n",
    "            data, channels, sampling_frequency = load_recording_data(recording_location)\n",
    "            utility_frequency = get_utility_frequency(recording_location + '.hea')\n",
    "            print(sampling_frequency)\n",
    "            print(utility_frequency)\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    # Extract features.\n",
    "    #return np.hstack((patient_features, eeg_features, ecg_features))\n",
    "    return np.hstack((patient_features, eeg_features_fourier))\n",
    "    #return np.hstack((eeg_features_fourier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f51186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(data):\n",
    "    \n",
    "    num_channels, num_samples = np.shape(data)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "952367a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the Challenge data...\n",
      "Extracting features and labels from the Challenge data...\n",
      "0286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r4/6g0q_vcj3w59527769q2l6jw0000gn/T/ipykernel_78198/2708894138.py:53: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode_val = np.real(stats.mode(delta_band_data_mean)[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Volumes/Bharadwaj/physionet-official/model_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_challenge_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 52\u001b[0m, in \u001b[0;36mtrain_challenge_model\u001b[0;34m(data_folder, model_folder, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, num_patients))\n\u001b[0;32m---> 52\u001b[0m current_features \u001b[38;5;241m=\u001b[39m \u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(current_features)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Extract labels.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 84\u001b[0m, in \u001b[0;36mget_features\u001b[0;34m(data_folder, patient_id)\u001b[0m\n\u001b[1;32m     82\u001b[0m recording_location \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_folder, patient_id, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(recording_id, group))\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(recording_location \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.hea\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 84\u001b[0m     data, channels, sampling_frequency \u001b[38;5;241m=\u001b[39m \u001b[43mload_recording_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecording_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(channel \u001b[38;5;129;01min\u001b[39;00m channels \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m eeg_channels):\n\u001b[1;32m     86\u001b[0m         data, channels \u001b[38;5;241m=\u001b[39m reduce_channels(data, channels, eeg_channels)\n",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m, in \u001b[0;36mload_recording_data\u001b[0;34m(record_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m head, tail \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(header_file)\n\u001b[1;32m     61\u001b[0m signal_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(head, \u001b[38;5;28mlist\u001b[39m(signal_files)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 62\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal_file\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Check that the dimensions of the signal data in the signal file is consistent with the dimensions for the signal data given\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# in the header file.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m num_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(channels)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:227\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    226\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 227\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m \u001b[43mMR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mdict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     mdict\u001b[38;5;241m.\u001b[39mupdate(matfile_dict)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/scipy/io/matlab/_mio4.py:404\u001b[0m, in \u001b[0;36mMatFile4Reader.get_variables\u001b[0;34m(self, variable_names)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmat_stream\u001b[38;5;241m.\u001b[39mseek(next_position)\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m mdict[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_var_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmat_stream\u001b[38;5;241m.\u001b[39mseek(next_position)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m variable_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/scipy/io/matlab/_mio4.py:379\u001b[0m, in \u001b[0;36mMatFile4Reader.read_var_array\u001b[0;34m(self, header, process)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_var_array\u001b[39m(\u001b[38;5;28mself\u001b[39m, header, process\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    364\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' Read array, given `header`\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m       `process`.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matrix_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_from_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/scipy/io/matlab/_mio4.py:141\u001b[0m, in \u001b[0;36mVarReader4.array_from_header\u001b[0;34m(self, hdr, process)\u001b[0m\n\u001b[1;32m    139\u001b[0m mclass \u001b[38;5;241m=\u001b[39m hdr\u001b[38;5;241m.\u001b[39mmclass\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mclass \u001b[38;5;241m==\u001b[39m mxFULL_CLASS:\n\u001b[0;32m--> 141\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_full_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mclass \u001b[38;5;241m==\u001b[39m mxCHAR_CLASS:\n\u001b[1;32m    143\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_char_array(hdr)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/scipy/io/matlab/_mio4.py:211\u001b[0m, in \u001b[0;36mVarReader4.read_full_array\u001b[0;34m(self, hdr)\u001b[0m\n\u001b[1;32m    209\u001b[0m     res_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_sub_array(hdr, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res \u001b[38;5;241m+\u001b[39m (res_j \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39mj)\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sub_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/scipy/io/matlab/_mio4.py:177\u001b[0m, in \u001b[0;36mVarReader4.read_sub_array\u001b[0;34m(self, hdr, copy)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dims:\n\u001b[1;32m    176\u001b[0m     num_bytes \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m d\n\u001b[0;32m--> 177\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmat_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m!=\u001b[39m num_bytes:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough bytes to read matrix \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; is this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma badly-formed file? Consider listing matrices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith `whosmat` and loading named matrices with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`variable_names` kwarg to `loadmat`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m hdr\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Parse the arguments.\n",
    "    if not (len(sys.argv) == 3 or len(sys.argv) == 4):\n",
    "        raise Exception('Include the data and model folders as arguments, e.g., python train_model.py data model.')\n",
    "\n",
    "    # Define the data and model foldes.\n",
    "    data_folder = \"/Volumes/Bharadwaj/physionet-official/data\"\n",
    "    model_folder = \"/Volumes/Bharadwaj/physionet-official/model_data\"\n",
    "    verbose = 1\n",
    "\n",
    "    train_challenge_model(data_folder, model_folder, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f986dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patient features from the data.\n",
    "def get_patient_features(data):\n",
    "    age = get_age(data)\n",
    "    sex = get_sex(data)\n",
    "    rosc = get_rosc(data)\n",
    "    ohca = get_ohca(data)\n",
    "    shockable_rhythm = get_shockable_rhythm(data)\n",
    "    ttm = get_ttm(data)\n",
    "\n",
    "    sex_features = np.zeros(2, dtype=int)\n",
    "    if sex == 'Female':\n",
    "        female = 1\n",
    "        male   = 0\n",
    "        other  = 0\n",
    "    elif sex == 'Male':\n",
    "        female = 0\n",
    "        male   = 1\n",
    "        other  = 0\n",
    "    else:\n",
    "        female = 0\n",
    "        male   = 0\n",
    "        other  = 1\n",
    "\n",
    "    features = np.array((age, female, male, other, rosc, ohca, shockable_rhythm, ttm))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f566c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the EEG data.\n",
    "def get_eeg_features(data, sampling_frequency):\n",
    "    num_channels, num_samples = np.shape(data)\n",
    "\n",
    "    if num_samples > 0:\n",
    "        delta_psd, _ = mne.time_frequency.psd_array_welch(data, sfreq=sampling_frequency,  fmin=0.5,  fmax=8.0, verbose=False)\n",
    "        theta_psd, _ = mne.time_frequency.psd_array_welch(data, sfreq=sampling_frequency,  fmin=4.0,  fmax=8.0, verbose=False)\n",
    "        alpha_psd, _ = mne.time_frequency.psd_array_welch(data, sfreq=sampling_frequency,  fmin=8.0, fmax=12.0, verbose=False)\n",
    "        beta_psd,  _ = mne.time_frequency.psd_array_welch(data, sfreq=sampling_frequency, fmin=12.0, fmax=30.0, verbose=False)\n",
    "\n",
    "        delta_psd_mean = np.nanmean(delta_psd, axis=1)\n",
    "        theta_psd_mean = np.nanmean(theta_psd, axis=1)\n",
    "        alpha_psd_mean = np.nanmean(alpha_psd, axis=1)\n",
    "        beta_psd_mean  = np.nanmean(beta_psd,  axis=1)\n",
    "    else:\n",
    "        delta_psd_mean = theta_psd_mean = alpha_psd_mean = beta_psd_mean = float('nan') * np.ones(num_channels)\n",
    "        \n",
    "    features = np.array((delta_psd_mean, theta_psd_mean, alpha_psd_mean, beta_psd_mean)).T\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cbdc6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the ECG data.\n",
    "def get_ecg_features(data):\n",
    "    num_channels, num_samples = np.shape(data)\n",
    "\n",
    "    if num_samples > 0:\n",
    "        mean = np.mean(data, axis=1)\n",
    "        std  = np.std(data, axis=1)\n",
    "    elif num_samples == 1:\n",
    "        mean = np.mean(data, axis=1)\n",
    "        std  = float('nan') * np.ones(num_channels)\n",
    "    else:\n",
    "        mean = float('nan') * np.ones(num_channels)\n",
    "        std = float('nan') * np.ones(num_channels)\n",
    "\n",
    "    features = np.array((mean, std)).T\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fb262f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fourier_features(data):\n",
    "    \n",
    "    num_channels, num_samples = np.shape(data)\n",
    "    \n",
    "    if num_samples > 0:\n",
    "    \n",
    "        signal = np.mean(data, axis=0)\n",
    "\n",
    "        # Assuming you have the EEG signal loaded in the 'signal' variable and a known sampling rate of 100 Hz\n",
    "        eeg_signal = signal.astype(np.float32)  # Convert to floating-point\n",
    "        sampling_rate = 100\n",
    "\n",
    "        # Calculate the length of the signal in seconds and frames\n",
    "        signal_duration = len(eeg_signal) / sampling_rate\n",
    "        n_frames = int(signal_duration * sampling_rate)\n",
    "\n",
    "        n_fft = 2048\n",
    "        hop_length = n_fft // 2  # You can adjust this value for overlap\n",
    "\n",
    "        # Initialize an array to store spectrogram data\n",
    "        spectrogram_data = []\n",
    "\n",
    "        # Process the signal in chunks\n",
    "        for i in range(0, len(eeg_signal) - n_frames + 1, n_frames):\n",
    "            chunk = eeg_signal[i:i+n_frames]\n",
    "            D = np.abs(librosa.stft(chunk, n_fft=n_fft, hop_length=hop_length))\n",
    "            spectrogram_data.append(D)\n",
    "\n",
    "        # Combine the spectrogram chunks into a single array\n",
    "        spectrogram = np.concatenate(spectrogram_data, axis=1)\n",
    "\n",
    "        # Load your mel-spectrogram data\n",
    "        # Replace 'mel_spectrogram' with your actual mel-spectrogram data\n",
    "        mel_spectrogram = spectrogram\n",
    "\n",
    "        # Calculate the Mel filterbank frequencies\n",
    "        mel_frequencies = librosa.mel_frequencies(n_mels=mel_spectrogram.shape[0], fmin=0, fmax=5)\n",
    "\n",
    "        # Find the indices corresponding to the delta band (0-5 Hz)\n",
    "        delta_indices = np.where((mel_frequencies >= 0) & (mel_frequencies <= 5))[0]\n",
    "\n",
    "        # Extract delta band data from the mel-spectrogram\n",
    "        delta_band_data = mel_spectrogram[delta_indices, :]\n",
    "        \n",
    "        delta_band_data_mean = np.mean(delta_band_data, axis = 0)\n",
    "\n",
    "        # features\n",
    "        mean_val = int(np.mean(delta_band_data_mean))\n",
    "        std_val = np.std(delta_band_data_mean)\n",
    "        min_val = int(np.min(delta_band_data_mean))\n",
    "        max_val = int(np.max(delta_band_data_mean))\n",
    "        median_val = int(np.median(delta_band_data_mean))\n",
    "        mode_val = np.real(stats.mode(delta_band_data_mean)[0])\n",
    "        mode_val = mode_val[0]\n",
    "        skewness_val = skew(delta_band_data_mean)\n",
    "        kurtosis_val = kurtosis(delta_band_data_mean)\n",
    "\n",
    "\n",
    "        features_stat = [mean_val, std_val, min_val, max_val, median_val, mode_val, skewness_val, kurtosis_val]\n",
    "        \n",
    "        \n",
    "        average_power = np.mean(delta_band_data, axis=0)\n",
    "        array_length = len(average_power)\n",
    "\n",
    "        # Calculate the desired window size\n",
    "        window_size = array_length // 30\n",
    "\n",
    "        # Reshape the array into windows\n",
    "        data_windows = np.array_split(average_power[:window_size * 30], 30)\n",
    "\n",
    "        # Calculate the mean within each window\n",
    "        window_averages = [window.mean() for window in data_windows]\n",
    "        \n",
    "        features = []\n",
    "        for i in window_averages:\n",
    "            features.append(i)\n",
    "        for j in features_stat:\n",
    "            features.append(j) \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        features = [0]*38\n",
    "        \n",
    "    features = np.array((features)).T\n",
    "    return features\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c3ce7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Edit this script to add your team's code. Some functions are *required*, but you can edit most parts of the required functions,\n",
    "# change or remove non-required functions, and add your own functions.\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# Optional libraries, functions, and variables. You can change or remove them.\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "import numpy as np, os, sys\n",
    "import mne\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# Required functions. Edit these functions to add your code, but do not change the arguments of the functions.\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "# Train your model.\n",
    "def train_challenge_model(data_folder, model_folder, verbose):\n",
    "    # Find data files.\n",
    "    if verbose >= 1:\n",
    "        print('Finding the Challenge data...')\n",
    "\n",
    "    patient_ids = find_data_folders(data_folder)\n",
    "    num_patients = len(patient_ids)\n",
    "\n",
    "    if num_patients==0:\n",
    "        raise FileNotFoundError('No data was provided.')\n",
    "\n",
    "    # Create a folder for the model if it does not already exist.\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "    # Extract the features and labels.\n",
    "    if verbose >= 1:\n",
    "        print('Extracting features and labels from the Challenge data...')\n",
    "\n",
    "    features = list()\n",
    "    outcomes = list()\n",
    "    cpcs = list()\n",
    "\n",
    "    for i in range(num_patients):\n",
    "        print(patient_ids[i])\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patients))\n",
    "\n",
    "        current_features = get_features(data_folder, patient_ids[i])\n",
    "        features.append(current_features)\n",
    "\n",
    "        # Extract labels.\n",
    "        patient_metadata = load_challenge_data(data_folder, patient_ids[i])\n",
    "        current_outcome = get_outcome(patient_metadata)\n",
    "        outcomes.append(current_outcome)\n",
    "        current_cpc = get_cpc(patient_metadata)\n",
    "        cpcs.append(current_cpc)\n",
    "       \n",
    "\n",
    "    features = np.vstack(features)\n",
    "    outcomes = np.vstack(outcomes)\n",
    "    cpcs = np.vstack(cpcs)\n",
    "   \n",
    "    df = pd.DataFrame(features)\n",
    "   \n",
    "\n",
    "    df.replace('nan', np.nan, inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "   \n",
    "   \n",
    "\n",
    "    features= df.to_numpy()\n",
    "\n",
    "    # # Train the models.\n",
    "    if verbose >= 1:\n",
    "        print('Training the Challenge model on the Challenge data...')\n",
    "\n",
    "    imputer = SimpleImputer(strategy='mean').fit(features)\n",
    "\n",
    "    # Train the models.\n",
    "    features = imputer.transform(features)\n",
    "    random_state = 42\n",
    "    \n",
    "    param_grid = {\n",
    "    'learning_rate': [0.01],\n",
    "    'max_depth': [5],\n",
    "    'max_leaf_nodes': [51],\n",
    "    'min_samples_leaf': [50],\n",
    "    'max_iter': [250],\n",
    "  \n",
    "}\n",
    "    # Train the HistGradientBoostingClassifier with GridSearchCV for hyperparameter tuning.\n",
    "    grid_search_clf = GridSearchCV(HistGradientBoostingClassifier(random_state=random_state),\n",
    "                                  param_grid=param_grid, cv=3)\n",
    "    grid_search_clf.fit(features, outcomes.ravel())\n",
    "    best_hist_gb_clf = grid_search_clf.best_estimator_\n",
    "   \n",
    "    # Train the HistGradientBoostingRegressor with GridSearchCV for hyperparameter tuning.\n",
    "    grid_search_reg = GridSearchCV(HistGradientBoostingRegressor(random_state=random_state),\n",
    "                                  param_grid=param_grid, cv=3)\n",
    "    grid_search_reg.fit(features, cpcs.ravel())\n",
    "    best_hist_gb_reg = grid_search_reg.best_estimator_\n",
    "    print(\"classifier :\",best_hist_gb_clf)\n",
    "    print(\"regressor :\",best_hist_gb_reg)\n",
    "   \n",
    "    # Fit the best parameters to the models.\n",
    "    best_hist_gb_clf.fit(features, outcomes.ravel())\n",
    "    best_hist_gb_reg.fit(features, cpcs.ravel())\n",
    "\n",
    "           \n",
    "\n",
    "    # Save the models.\n",
    "    save_challenge_model(model_folder, imputer, best_hist_gb_clf, best_hist_gb_reg)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')\n",
    "\n",
    "# Load your trained models. This function is required. You should edit this function to add your code, but do not change the\n",
    "# arguments of this function.\n",
    "def load_challenge_models(model_folder, verbose):\n",
    "    filename = os.path.join(model_folder, 'models.sav')\n",
    "    return joblib.load(filename)\n",
    "\n",
    "# Run your trained models. This function is required. You should edit this function to add your code, but do not change the\n",
    "# arguments of this function.\n",
    "def run_challenge_models(models, data_folder, patient_id, verbose):\n",
    "    imputer = models['imputer']\n",
    "    outcome_model = models['outcome_model']\n",
    "    cpc_model = models['cpc_model']\n",
    "\n",
    "    # Extract features.\n",
    "    features = get_features(data_folder, patient_id)\n",
    "    features = features.reshape(1, -1)\n",
    "   \n",
    "    df1 = pd.DataFrame(features)\n",
    "\n",
    "    df1.replace('nan', np.nan, inplace=True)\n",
    "\n",
    "    # Convert columns to numeric (necessary for mean calculation)\n",
    "    df1 = df1.apply(pd.to_numeric, errors='ignore')\n",
    "   \n",
    "    #print(df)\n",
    "\n",
    "   \n",
    "\n",
    "    # Replace NaN values with column means\n",
    "    df_filled1 = df1.fillna(0)\n",
    "\n",
    "    features= df_filled1.to_numpy()\n",
    "\n",
    "    # Impute missing data.\n",
    "    features = imputer.transform(features)\n",
    "\n",
    "    # Apply models to features.\n",
    "    outcome = outcome_model.predict(features)[0]\n",
    "    outcome_probability = outcome_model.predict_proba(features)[0, 1]\n",
    "    cpc = cpc_model.predict(features)[0]\n",
    "\n",
    "    # Ensure that the CPC score is between (or equal to) 1 and 5.\n",
    "    cpc = np.clip(cpc, 1, 5)\n",
    "\n",
    "    return outcome, outcome_probability, cpc\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# Optional functions. You can change or remove these functions and/or add new functions.\n",
    "#\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4522a967",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the Challenge data...\n",
      "Extracting features and labels from the Challenge data...\n",
      "0286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r4/6g0q_vcj3w59527769q2l6jw0000gn/T/ipykernel_78198/2708894138.py:53: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode_val = np.real(stats.mode(delta_band_data_mean)[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0286_001_020', '0286_001_020', '0286_002_021', '0286_002_021', '0286_003_022', '0286_003_022', '0286_004_023', '0286_004_023', '0286_005_024', '0286_005_024', '0286_006_025', '0286_006_025', '0286_007_025', '0286_007_025', '0286_008_026', '0286_008_026', '0286_009_027', '0286_009_027', '0286_010_028', '0286_010_028', '0286_011_029', '0286_011_029', '0286_012_030', '0286_012_030', '0286_013_030', '0286_013_030', '0286_014_031', '0286_014_031', '0286_015_032', '0286_015_032', '0286_016_033', '0286_016_033', '0286_017_034', '0286_017_034', '0286_018_035', '0286_018_035', '0286_019_036', '0286_019_036', '0286_020_036', '0286_020_036', '0286_021_037', '0286_021_037', '0286_022_038', '0286_022_038', '0286_023_039', '0286_023_039', '0286_024_040', '0286_024_040', '0286_025_040', '0286_025_040', '0286_026_041', '0286_026_041', '0286_027_042', '0286_027_042', '0286_028_043', '0286_028_043', '0286_029_044', '0286_029_044', '0286_030_045', '0286_030_045', '0286_031_045', '0286_031_045', '0286_032_046', '0286_032_046', '0286_033_047', '0286_033_047', '0286_034_048', '0286_034_048', '0286_035_049', '0286_035_049', '0286_036_050', '0286_036_050', '0286_037_050', '0286_037_050', '0286_038_051', '0286_038_051', '0286_039_052', '0286_039_052', '0286_040_053', '0286_040_053', '0286_041_054', '0286_041_054', '0286_042_055', '0286_042_055', '0286_043_056', '0286_043_056', '0286_044_056', '0286_044_056', '0286_045_057', '0286_045_057', '0286_046_058', '0286_046_058', '0286_047_059', '0286_047_059', '0286_048_060', '0286_048_060', '0286_049_061', '0286_049_061', '0286_050_061', '0286_050_061', '0286_051_062', '0286_051_062', '0286_052_063', '0286_052_063', '0286_053_064', '0286_053_064', '0286_054_064', '0286_054_064', '0286_055_065', '0286_055_065', '0286_056_066', '0286_056_066', '0286_057_067', '0286_057_067', '0286_058_068', '0286_058_068', '0286_059_069', '0286_059_069', '0286_060_069', '0286_060_069', '0286_061_070', '0286_061_070', '0286_062_071', '0286_062_071', '0286_063_072', '0286_063_072']\n",
      "/Volumes/Bharadwaj/physionet-official/data/0286/0286_063_072_EEG\n",
      "512.0\n",
      "None\n",
      "0296\n",
      "['0296_001_002', '0296_001_002', '0296_001_002', '0296_002_003', '0296_002_003', '0296_002_003', '0296_003_004', '0296_003_004', '0296_003_004', '0296_004_005', '0296_004_005', '0296_004_005', '0296_005_006', '0296_005_006', '0296_005_006', '0296_006_006', '0296_006_006', '0296_006_006']\n",
      "/Volumes/Bharadwaj/physionet-official/data/0296/0296_006_006_EEG\n",
      "500.0\n",
      "None\n",
      "0346\n",
      "['0346_001_015', '0346_001_015', '0346_001_015', '0346_001_015', '0346_002_016', '0346_002_016', '0346_002_016', '0346_002_016', '0346_003_017', '0346_003_017', '0346_003_017', '0346_003_017', '0346_004_018', '0346_004_018', '0346_004_018', '0346_004_018', '0346_005_019', '0346_005_019', '0346_005_019', '0346_005_019', '0346_006_020', '0346_006_020', '0346_006_020', '0346_006_020', '0346_007_021', '0346_007_021', '0346_007_021', '0346_007_021', '0346_008_022', '0346_008_022', '0346_008_022', '0346_008_022', '0346_009_023', '0346_009_023', '0346_009_023', '0346_009_023', '0346_010_024', '0346_010_024', '0346_010_024', '0346_010_024', '0346_011_025', '0346_011_025', '0346_011_025', '0346_011_025', '0346_012_026', '0346_012_026', '0346_012_026', '0346_012_026', '0346_013_027', '0346_013_027', '0346_013_027', '0346_013_027', '0346_014_028', '0346_014_028', '0346_014_028', '0346_014_028', '0346_015_029', '0346_015_029', '0346_015_029', '0346_015_029', '0346_016_030', '0346_016_030', '0346_016_030', '0346_016_030', '0346_017_031', '0346_017_031', '0346_017_031', '0346_017_031', '0346_018_032', '0346_018_032', '0346_018_032', '0346_018_032', '0346_019_033', '0346_019_033', '0346_019_033', '0346_019_033', '0346_020_034', '0346_020_034', '0346_020_034', '0346_020_034', '0346_021_035', '0346_021_035', '0346_021_035', '0346_021_035', '0346_022_036', '0346_022_036', '0346_022_036', '0346_022_036', '0346_023_037', '0346_023_037', '0346_023_037', '0346_023_037', '0346_024_038', '0346_024_038', '0346_024_038', '0346_024_038']\n",
      "/Volumes/Bharadwaj/physionet-official/data/0346/0346_024_038_EEG\n",
      "256.0\n",
      "None\n",
      "0363\n",
      "['0363_001_006', '0363_001_006', '0363_002_007', '0363_002_007', '0363_003_008', '0363_003_008', '0363_004_009', '0363_004_009', '0363_005_010', '0363_005_010', '0363_006_011', '0363_006_011', '0363_007_012', '0363_007_012', '0363_008_013', '0363_008_013', '0363_009_014', '0363_009_014', '0363_010_015', '0363_010_015', '0363_011_016', '0363_011_016', '0363_012_017', '0363_012_017', '0363_013_018', '0363_013_018', '0363_014_019', '0363_014_019', '0363_015_020', '0363_015_020', '0363_016_021', '0363_016_021', '0363_017_022', '0363_017_022', '0363_018_023', '0363_018_023', '0363_019_024', '0363_019_024', '0363_020_025', '0363_020_025', '0363_021_026', '0363_021_026', '0363_022_027', '0363_022_027', '0363_023_028', '0363_023_028', '0363_024_029', '0363_024_029', '0363_025_030', '0363_025_030', '0363_026_031', '0363_026_031', '0363_027_032', '0363_027_032', '0363_028_033', '0363_028_033', '0363_029_034', '0363_029_034', '0363_030_035', '0363_030_035', '0363_031_036', '0363_031_036', '0363_032_037', '0363_032_037', '0363_033_038', '0363_033_038', '0363_034_039', '0363_034_039', '0363_035_040', '0363_035_040', '0363_036_041', '0363_036_041', '0363_037_042', '0363_037_042', '0363_038_043', '0363_038_043', '0363_039_044', '0363_039_044', '0363_040_045', '0363_040_045', '0363_041_046', '0363_041_046', '0363_042_047', '0363_042_047', '0363_043_048', '0363_043_048', '0363_044_049', '0363_044_049', '0363_045_050', '0363_045_050', '0363_046_051', '0363_046_051', '0363_047_052', '0363_047_052', '0363_048_053', '0363_048_053', '0363_049_054', '0363_049_054', '0363_050_055', '0363_050_055', '0363_051_056', '0363_051_056', '0363_052_057', '0363_052_057', '0363_053_058', '0363_053_058', '0363_054_059', '0363_054_059', '0363_055_060', '0363_055_060', '0363_056_061', '0363_056_061', '0363_057_062', '0363_057_062', '0363_058_063', '0363_058_063', '0363_059_064', '0363_059_064', '0363_060_065', '0363_060_065', '0363_061_066', '0363_061_066', '0363_062_067', '0363_062_067', '0363_063_068', '0363_063_068', '0363_064_069', '0363_064_069', '0363_065_070', '0363_065_070', '0363_066_071', '0363_066_071', '0363_067_072', '0363_067_072', '0363_068_073', '0363_068_073', '0363_069_074', '0363_069_074', '0363_070_075', '0363_070_075', '0363_071_076', '0363_071_076', '0363_072_077', '0363_072_077', '0363_073_078', '0363_073_078', '0363_074_079', '0363_074_079', '0363_075_080', '0363_075_080', '0363_076_081', '0363_076_081', '0363_077_082', '0363_077_082', '0363_078_083', '0363_078_083', '0363_079_084', '0363_079_084', '0363_080_085', '0363_080_085', '0363_081_086', '0363_081_086', '0363_082_087', '0363_082_087', '0363_083_088', '0363_083_088', '0363_084_089', '0363_084_089', '0363_085_090', '0363_085_090', '0363_086_091', '0363_086_091', '0363_087_092', '0363_087_092', '0363_088_093', '0363_088_093', '0363_089_094', '0363_089_094', '0363_090_095', '0363_090_095', '0363_091_096', '0363_091_096', '0363_092_097', '0363_092_097', '0363_093_098', '0363_093_098', '0363_094_099', '0363_094_099', '0363_095_100', '0363_095_100', '0363_096_101', '0363_096_101', '0363_097_102', '0363_097_102', '0363_098_103', '0363_098_103', '0363_099_104', '0363_099_104', '0363_100_105', '0363_100_105', '0363_101_106', '0363_101_106', '0363_102_107', '0363_102_107', '0363_103_108', '0363_103_108', '0363_104_109', '0363_104_109', '0363_105_110', '0363_105_110', '0363_106_111', '0363_106_111', '0363_107_112', '0363_107_112', '0363_108_113', '0363_108_113', '0363_109_114', '0363_109_114', '0363_110_115', '0363_110_115', '0363_111_116', '0363_111_116', '0363_112_117', '0363_112_117', '0363_113_118', '0363_113_118', '0363_114_119', '0363_114_119', '0363_115_120', '0363_115_120', '0363_116_121', '0363_116_121', '0363_117_122', '0363_117_122', '0363_118_123', '0363_118_123', '0363_119_124', '0363_119_124', '0363_120_125', '0363_120_125', '0363_121_126', '0363_121_126', '0363_122_127', '0363_122_127', '0363_123_128', '0363_123_128', '0363_124_129', '0363_124_129', '0363_125_130', '0363_125_130', '0363_126_131', '0363_126_131', '0363_127_132', '0363_127_132', '0363_128_133', '0363_128_133', '0363_129_134', '0363_129_134', '0363_130_135', '0363_130_135', '0363_131_136', '0363_131_136', '0363_132_137', '0363_132_137', '0363_133_138', '0363_133_138', '0363_134_139', '0363_134_139', '0363_135_140', '0363_135_140', '0363_136_141', '0363_136_141', '0363_137_142', '0363_137_142', '0363_138_143', '0363_138_143', '0363_139_144', '0363_139_144', '0363_140_145', '0363_140_145', '0363_141_146', '0363_141_146', '0363_142_147', '0363_142_147', '0363_143_148', '0363_143_148', '0363_144_149', '0363_144_149', '0363_145_150', '0363_145_150', '0363_146_151', '0363_146_151', '0363_147_152', '0363_147_152', '0363_148_153', '0363_148_153', '0363_149_154', '0363_149_154', '0363_150_155', '0363_150_155', '0363_151_156', '0363_151_156', '0363_152_157', '0363_152_157', '0363_153_158', '0363_153_158', '0363_154_159', '0363_154_159', '0363_155_160', '0363_155_160', '0363_156_161', '0363_156_161', '0363_157_162', '0363_157_162', '0363_158_163', '0363_158_163', '0363_159_164', '0363_159_164', '0363_160_165', '0363_160_165', '0363_161_166', '0363_161_166', '0363_162_167', '0363_162_167', '0363_163_168', '0363_163_168', '0363_164_169', '0363_164_169', '0363_165_170', '0363_165_170', '0363_166_171', '0363_166_171', '0363_167_172', '0363_167_172', '0363_168_173', '0363_168_173', '0363_169_174', '0363_169_174', '0363_170_175', '0363_170_175', '0363_171_176', '0363_171_176', '0363_172_177', '0363_172_177', '0363_173_178', '0363_173_178', '0363_174_179', '0363_174_179', '0363_175_180', '0363_175_180', '0363_176_181', '0363_176_181', '0363_177_182', '0363_177_182', '0363_178_183', '0363_178_183', '0363_179_184', '0363_179_184', '0363_180_185', '0363_180_185', '0363_181_186', '0363_181_186', '0363_182_187', '0363_182_187', '0363_183_188', '0363_183_188', '0363_184_189', '0363_184_189', '0363_185_190', '0363_185_190', '0363_186_191', '0363_186_191', '0363_187_192', '0363_187_192', '0363_188_193', '0363_188_193', '0363_189_194', '0363_189_194', '0363_190_195', '0363_190_195', '0363_191_196', '0363_191_196', '0363_192_197', '0363_192_197', '0363_193_198', '0363_193_198', '0363_194_199', '0363_194_199', '0363_195_200', '0363_195_200', '0363_196_201', '0363_196_201', '0363_197_202', '0363_197_202', '0363_198_203', '0363_198_203', '0363_199_204', '0363_199_204', '0363_200_205', '0363_200_205', '0363_201_206', '0363_201_206', '0363_202_207', '0363_202_207', '0363_203_208', '0363_203_208', '0363_204_209', '0363_204_209', '0363_205_210', '0363_205_210', '0363_206_211', '0363_206_211', '0363_207_212', '0363_207_212', '0363_208_213', '0363_208_213', '0363_209_214', '0363_209_214', '0363_210_215', '0363_210_215', '0363_211_216', '0363_211_216', '0363_212_217', '0363_212_217', '0363_213_218', '0363_213_218', '0363_214_219', '0363_214_219', '0363_215_220', '0363_215_220', '0363_216_221', '0363_216_221', '0363_217_222', '0363_217_222', '0363_218_223', '0363_218_223', '0363_219_224', '0363_219_224', '0363_220_225', '0363_220_225', '0363_221_226', '0363_221_226', '0363_222_227', '0363_222_227', '0363_223_228', '0363_223_228', '0363_224_229', '0363_224_229', '0363_225_230', '0363_225_230', '0363_226_231', '0363_226_231', '0363_227_232', '0363_227_232', '0363_228_233', '0363_228_233', '0363_229_234', '0363_229_234', '0363_230_235', '0363_230_235', '0363_231_236', '0363_231_236', '0363_232_237', '0363_232_237', '0363_233_238', '0363_233_238', '0363_234_239', '0363_234_239', '0363_235_240', '0363_235_240', '0363_236_241', '0363_236_241', '0363_237_242', '0363_237_242', '0363_238_243', '0363_238_243', '0363_239_244', '0363_239_244', '0363_240_245', '0363_240_245', '0363_241_246', '0363_241_246', '0363_242_247', '0363_242_247', '0363_243_248', '0363_243_248', '0363_244_249', '0363_244_249', '0363_245_250', '0363_245_250', '0363_246_251', '0363_246_251', '0363_247_252', '0363_247_252', '0363_248_253', '0363_248_253', '0363_249_254', '0363_249_254']\n",
      "/Volumes/Bharadwaj/physionet-official/data/0363/0363_249_254_EEG\n",
      "250.0\n",
      "None\n",
      "Training the Challenge model on the Challenge data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bharath/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "/Users/bharath/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/bharath/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/bharath/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier : HistGradientBoostingClassifier(learning_rate=0.01, max_depth=5, max_iter=250,\n",
      "                               max_leaf_nodes=51, min_samples_leaf=50,\n",
      "                               random_state=42)\n",
      "regressor : HistGradientBoostingRegressor(learning_rate=0.01, max_depth=5, max_iter=250,\n",
      "                              max_leaf_nodes=51, min_samples_leaf=50,\n",
      "                              random_state=42)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Do *not* edit this script. Changes will be discarded so that we can train the models consistently.\n",
    "\n",
    "# This file contains functions for training models for the Challenge. You can run it as follows:\n",
    "#\n",
    "#   python train_model.py data model\n",
    "#\n",
    "# where 'data' is a folder containing the Challenge data and 'model' is a folder for saving your model.\n",
    "\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parse the arguments.\n",
    "    if not (len(sys.argv) == 3 or len(sys.argv) == 4):\n",
    "        raise Exception('Include the data and model folders as arguments, e.g., python train_model.py data model.')\n",
    "\n",
    "    # Define the data and model foldes.\n",
    "    data_folder = \"/Volumes/Bharadwaj/physionet-official/data\"\n",
    "    model_folder = \"/Volumes/Bharadwaj/physionet-official/model_data\"\n",
    "    verbose = 1\n",
    "\n",
    "    train_challenge_model(data_folder, model_folder, verbose) ### Teams: Implement this function!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb6e77",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Do *not* edit this script. Changes will be discarded so that we can run the trained models consistently.\n",
    "\n",
    "# This file contains functions for running models for the Challenge. You can run it as follows:\n",
    "#\n",
    "#   python run_model.py models data outputs\n",
    "#\n",
    "# where 'models' is a folder containing the your trained models, 'data' is a folder containing the Challenge data, and 'outputs' is a\n",
    "# folder for saving your models' outputs.\n",
    "\n",
    "import numpy as np, scipy as sp, os, sys\n",
    "\n",
    "# Run model.\n",
    "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose):\n",
    "    # Load model(s).\n",
    "    if verbose >= 1:\n",
    "        print('Loading the Challenge models...')\n",
    "\n",
    "    # You can use this function to perform tasks, such as loading your models, that you only need to perform once.\n",
    "    models = load_challenge_models(model_folder, verbose) ### Teams: Implement this function!!!\n",
    "\n",
    "    # Find the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Finding the Challenge data...')\n",
    "\n",
    "    patient_ids = find_data_folders(data_folder)\n",
    "    num_patients = len(patient_ids)\n",
    "\n",
    "    if num_patients==0:\n",
    "        raise Exception('No data were provided.')\n",
    "\n",
    "    # Create a folder for the Challenge outputs if it does not already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run the team's model(s) on the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Running the Challenge models on the Challenge data...')\n",
    "\n",
    "    # Iterate over the patients.\n",
    "    for i in range(num_patients):\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patients))\n",
    "\n",
    "        patient_id = patient_ids[i]\n",
    "\n",
    "        # Allow or disallow the model(s) to fail on parts of the data; this can be helpful for debugging.\n",
    "        try:\n",
    "            outcome_binary, outcome_probability, cpc = run_challenge_models(models, data_folder, patient_id, verbose) ### Teams: Implement this function!!!\n",
    "        except:\n",
    "            if allow_failures:\n",
    "                if verbose >= 2:\n",
    "                    print('... failed.')\n",
    "                outcome_binary, outcome_probability, cpc = float('nan'), float('nan'), float('nan')\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Save Challenge outputs.\n",
    "        os.makedirs(os.path.join(output_folder, patient_id), exist_ok=True)\n",
    "        output_file = os.path.join(output_folder, patient_id, patient_id + '.txt')\n",
    "        save_challenge_outputs(output_file, patient_id, outcome_binary, outcome_probability, cpc)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53939610",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Parse the arguments.\n",
    "\n",
    "    # Define the model, data, and output folders.\n",
    "    model_folder = \"/Volumes/Bharadwaj/physionet-official/model_data\"\n",
    "    data_folder = \"/Volumes/Bharadwaj/physionet-official/test\"\n",
    "    output_folder = \"/Volumes/Bharadwaj/physionet-official/output\"\n",
    "\n",
    "    # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
    "    allow_failures = False\n",
    "\n",
    "    # Change the level of verbosity; helpful for debugging.\n",
    "\n",
    "    verbose = 1\n",
    "\n",
    "    run_model(model_folder, data_folder, output_folder, allow_failures, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42f5a4",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9073b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Do *not* edit this script. Changes will be discarded so that we can process the models consistently.\n",
    "\n",
    "# This file contains functions for evaluating models for the Challenge. You can run it as follows:\n",
    "#\n",
    "#   python evaluate_model.py labels outputs scores.csv\n",
    "#\n",
    "# where 'labels' is a folder containing files with the labels, 'outputs' is a folder containing files with the outputs from your\n",
    "# model, and 'scores.csv' (optional) is a collection of scores for the model outputs.\n",
    "#\n",
    "# Each label or output file must have the format described on the Challenge webpage. The scores for the algorithm outputs are also\n",
    "# described on the Challenge webpage.\n",
    "\n",
    "import os, os.path, sys, numpy as np\n",
    "\n",
    "# Evaluate the models.\n",
    "def evaluate_model(label_folder, output_folder):\n",
    "    # Load the labels.\n",
    "    patient_ids = find_data_folders(label_folder)\n",
    "    num_patients = len(patient_ids)\n",
    "\n",
    "    hospitals = list()\n",
    "    label_outcomes = list()\n",
    "    label_cpcs = list()\n",
    "\n",
    "    for i in range(num_patients):\n",
    "        patient_data_file = os.path.join(label_folder, patient_ids[i], patient_ids[i] + '.txt')\n",
    "        patient_data = load_text_file(patient_data_file)\n",
    "\n",
    "        hospital = get_hospital(patient_data)\n",
    "        label_outcome = get_outcome(patient_data)\n",
    "        label_cpc = get_cpc(patient_data)\n",
    "\n",
    "        hospitals.append(hospital)\n",
    "        label_outcomes.append(label_outcome)\n",
    "        label_cpcs.append(label_cpc)\n",
    "\n",
    "    # Load the model outputs.\n",
    "    output_outcomes = list()\n",
    "    output_outcome_probabilities = list()\n",
    "    output_cpcs = list()\n",
    "\n",
    "    for i in range(num_patients):\n",
    "        output_file = os.path.join(output_folder, patient_ids[i], patient_ids[i] + '.txt')\n",
    "        output_data = load_text_file(output_file)\n",
    "\n",
    "        output_outcome = get_outcome(output_data)\n",
    "        output_outcome_probability = get_outcome_probability(output_data)\n",
    "        output_cpc = get_cpc(output_data)\n",
    "\n",
    "        output_outcomes.append(output_outcome)\n",
    "        output_outcome_probabilities.append(output_outcome_probability)\n",
    "        output_cpcs.append(output_cpc)\n",
    "\n",
    "    # Evaluate the models.\n",
    "    challenge_score = compute_challenge_score(label_outcomes, output_outcome_probabilities, hospitals)\n",
    "    auroc_outcomes, auprc_outcomes = compute_auc(label_outcomes, output_outcome_probabilities)\n",
    "    accuracy_outcomes, _, _ = compute_accuracy(label_outcomes, output_outcomes)\n",
    "    f_measure_outcomes, _, _ = compute_f_measure(label_outcomes, output_outcomes)\n",
    "    mse_cpcs = compute_mse(label_cpcs, output_cpcs)\n",
    "    mae_cpcs = compute_mae(label_cpcs, output_cpcs)\n",
    "\n",
    "    # Return the results.\n",
    "    return challenge_score, auroc_outcomes, auprc_outcomes, accuracy_outcomes, f_measure_outcomes, mse_cpcs, mae_cpcs\n",
    "\n",
    "# Compute the Challenge score.\n",
    "def compute_challenge_score(labels, outputs, hospitals):\n",
    "    # Check the data.\n",
    "    assert len(labels) == len(outputs)\n",
    "\n",
    "    # Convert the data to NumPy arrays for easier indexing.\n",
    "    labels = np.asarray(labels, dtype=np.float64)\n",
    "    outputs = np.asarray(outputs, dtype=np.float64)\n",
    "\n",
    "    # Identify the unique hospitals.\n",
    "    unique_hospitals = sorted(set(hospitals))\n",
    "    num_hospitals = len(unique_hospitals)\n",
    "\n",
    "    # Initialize a confusion matrix for each hospital.\n",
    "    tps = np.zeros(num_hospitals)\n",
    "    fps = np.zeros(num_hospitals)\n",
    "    fns = np.zeros(num_hospitals)\n",
    "    tns = np.zeros(num_hospitals)\n",
    "\n",
    "    # Compute the confusion matrix at each output threshold separately for each hospital.\n",
    "    for i, hospital in enumerate(unique_hospitals):\n",
    "        idx = [j for j, x in enumerate(hospitals) if x == hospital]\n",
    "        current_labels = labels[idx]\n",
    "        current_outputs = outputs[idx]\n",
    "        num_instances = len(current_labels)\n",
    "\n",
    "        # Collect the unique output values as the thresholds for the positive and negative classes.\n",
    "        thresholds = np.unique(current_outputs)\n",
    "        thresholds = np.append(thresholds, thresholds[-1]+1)\n",
    "        thresholds = thresholds[::-1]\n",
    "        num_thresholds = len(thresholds)\n",
    "\n",
    "        idx = np.argsort(current_outputs)[::-1]\n",
    "\n",
    "        # Initialize the TPs, FPs, FNs, and TNs with no positive outputs.\n",
    "        tp = np.zeros(num_thresholds)\n",
    "        fp = np.zeros(num_thresholds)\n",
    "        fn = np.zeros(num_thresholds)\n",
    "        tn = np.zeros(num_thresholds)\n",
    "\n",
    "        tp[0] = 0\n",
    "        fp[0] = 0\n",
    "        fn[0] = np.sum(current_labels == 1)\n",
    "        tn[0] = np.sum(current_labels == 0)\n",
    "\n",
    "        # Update the TPs, FPs, FNs, and TNs using the values at the previous threshold.\n",
    "        k = 0\n",
    "        for l in range(1, num_thresholds):\n",
    "            tp[l] = tp[l-1]\n",
    "            fp[l] = fp[l-1]\n",
    "            fn[l] = fn[l-1]\n",
    "            tn[l] = tn[l-1]\n",
    "\n",
    "            while k < num_instances and current_outputs[idx[k]] >= thresholds[l]:\n",
    "                if current_labels[idx[k]] == 1:\n",
    "                    tp[l] += 1\n",
    "                    fn[l] -= 1\n",
    "                else:\n",
    "                    fp[l] += 1\n",
    "                    tn[l] -= 1\n",
    "                k += 1\n",
    "\n",
    "            # Compute the FPRs.\n",
    "            fpr = np.zeros(num_thresholds)\n",
    "            for l in range(num_thresholds):\n",
    "                if tp[l] + fn[l] > 0:\n",
    "                    fpr[l] = float(fp[l]) / float(tp[l] + fn[l])\n",
    "                else:\n",
    "                    fpr[l] = float('nan')\n",
    "\n",
    "            # Find the threshold such that FPR <= 0.05.\n",
    "            max_fpr = 0.05\n",
    "            if np.any(fpr <= max_fpr):\n",
    "                l = max(l for l, x in enumerate(fpr) if x <= max_fpr)\n",
    "                tps[i] = tp[l]\n",
    "                fps[i] = fp[l]\n",
    "                fns[i] = fn[l]\n",
    "                tns[i] = tn[l]\n",
    "            else:\n",
    "                tps[i] = tp[0]\n",
    "                fps[i] = fp[0]\n",
    "                fns[i] = fn[0]\n",
    "                tns[i] = tn[0]\n",
    "\n",
    "    # Compute the TPR at FPR <= 0.05 for each hospital.\n",
    "    tp = np.sum(tps)\n",
    "    fp = np.sum(fps)\n",
    "    fn = np.sum(fns)\n",
    "    tn = np.sum(tns)\n",
    "\n",
    "    if tp + fn > 0:\n",
    "        max_tpr = tp / (tp + fn)\n",
    "    else:\n",
    "        max_tpr = float('nan')\n",
    "\n",
    "    return max_tpr\n",
    "\n",
    "# Compute area under the receiver operating characteristic curve (AUROC) and area under the precision recall curve (AUPRC).\n",
    "def compute_auc(labels, outputs):\n",
    "    assert len(labels) == len(outputs)\n",
    "    num_instances = len(labels)\n",
    "\n",
    "    # Convert the data to NumPy arrays for easier indexing.\n",
    "    labels = np.asarray(labels, dtype=np.float64)\n",
    "    outputs = np.asarray(outputs, dtype=np.float64)\n",
    "\n",
    "    # Collect the unique output values as the thresholds for the positive and negative classes.\n",
    "    thresholds = np.unique(outputs)\n",
    "    thresholds = np.append(thresholds, thresholds[-1]+1)\n",
    "    thresholds = thresholds[::-1]\n",
    "    num_thresholds = len(thresholds)\n",
    "\n",
    "    idx = np.argsort(outputs)[::-1]\n",
    "\n",
    "    # Initialize the TPs, FPs, FNs, and TNs with no positive outputs.\n",
    "    tp = np.zeros(num_thresholds)\n",
    "    fp = np.zeros(num_thresholds)\n",
    "    fn = np.zeros(num_thresholds)\n",
    "    tn = np.zeros(num_thresholds)\n",
    "\n",
    "    tp[0] = 0\n",
    "    fp[0] = 0\n",
    "    fn[0] = np.sum(labels == 1)\n",
    "    tn[0] = np.sum(labels == 0)\n",
    "\n",
    "    # Update the TPs, FPs, FNs, and TNs using the values at the previous threshold.\n",
    "    i = 0\n",
    "    for j in range(1, num_thresholds):\n",
    "        tp[j] = tp[j-1]\n",
    "        fp[j] = fp[j-1]\n",
    "        fn[j] = fn[j-1]\n",
    "        tn[j] = tn[j-1]\n",
    "\n",
    "        while i < num_instances and outputs[idx[i]] >= thresholds[j]:\n",
    "            if labels[idx[i]] == 1:\n",
    "                tp[j] += 1\n",
    "                fn[j] -= 1\n",
    "            else:\n",
    "                fp[j] += 1\n",
    "                tn[j] -= 1\n",
    "            i += 1\n",
    "\n",
    "    # Compute the TPRs, TNRs, and PPVs at each threshold.\n",
    "    tpr = np.zeros(num_thresholds)\n",
    "    tnr = np.zeros(num_thresholds)\n",
    "    ppv = np.zeros(num_thresholds)\n",
    "    for j in range(num_thresholds):\n",
    "        if tp[j] + fn[j] > 0:\n",
    "            tpr[j] = tp[j] / (tp[j] + fn[j])\n",
    "        else:\n",
    "            tpr[j] = float('nan')\n",
    "        if fp[j] + tn[j] > 0:\n",
    "            tnr[j] = tn[j] / (fp[j] + tn[j])\n",
    "        else:\n",
    "            tnr[j] = float('nan')\n",
    "        if tp[j] + fp[j] > 0:\n",
    "            ppv[j] = tp[j] / (tp[j] + fp[j])\n",
    "        else:\n",
    "            ppv[j] = float('nan')\n",
    "\n",
    "    # Compute AUROC as the area under a piecewise linear function with TPR/sensitivity (x-axis) and TNR/specificity (y-axis) and\n",
    "    # AUPRC as the area under a piecewise constant with TPR/recall (x-axis) and PPV/precision (y-axis).\n",
    "    auroc = 0.0\n",
    "    auprc = 0.0\n",
    "    for j in range(num_thresholds-1):\n",
    "        auroc += 0.5 * (tpr[j+1] - tpr[j]) * (tnr[j+1] + tnr[j])\n",
    "        auprc += (tpr[j+1] - tpr[j]) * ppv[j+1]\n",
    "\n",
    "    return auroc, auprc\n",
    "\n",
    "# Construct the one-hot encoding of data for the given classes.\n",
    "def compute_one_hot_encoding(data, classes):\n",
    "    num_instances = len(data)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    one_hot_encoding = np.zeros((num_instances, num_classes), dtype=np.bool_)\n",
    "    unencoded_data = list()\n",
    "    for i, x in enumerate(data):\n",
    "        for j, y in enumerate(classes):\n",
    "            if (x == y) or (is_nan(x) and is_nan(y)):\n",
    "                one_hot_encoding[i, j] = 1\n",
    "\n",
    "    return one_hot_encoding\n",
    "\n",
    "# Compute the binary confusion matrix, where the columns are the expert labels and the rows are the classifier labels for the given\n",
    "# classes.\n",
    "def compute_confusion_matrix(labels, outputs, classes):\n",
    "    assert np.shape(labels) == np.shape(outputs)\n",
    "\n",
    "    num_instances = len(labels)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    A = np.zeros((num_classes, num_classes))\n",
    "    for k in range(num_instances):\n",
    "        for i in range(num_classes):\n",
    "            for j in range(num_classes):\n",
    "                if outputs[k, i] == 1 and labels[k, j] == 1:\n",
    "                    A[i, j] += 1\n",
    "\n",
    "    return A\n",
    "\n",
    "# Construct the binary one-vs-rest confusion matrices, where the columns are the expert labels and the rows are the classifier\n",
    "# for the given classes.\n",
    "def compute_one_vs_rest_confusion_matrix(labels, outputs, classes):\n",
    "    assert np.shape(labels) == np.shape(outputs)\n",
    "\n",
    "    num_instances = len(labels)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    A = np.zeros((num_classes, 2, 2))\n",
    "    for i in range(num_instances):\n",
    "        for j in range(num_classes):\n",
    "            if labels[i, j] == 1 and outputs[i, j] == 1: # TP\n",
    "                A[j, 0, 0] += 1\n",
    "            elif labels[i, j] == 0 and outputs[i, j] == 1: # FP\n",
    "                A[j, 0, 1] += 1\n",
    "            elif labels[i, j] == 1 and outputs[i, j] == 0: # FN\n",
    "                A[j, 1, 0] += 1\n",
    "            elif labels[i, j] == 0 and outputs[i, j] == 0: # TN\n",
    "                A[j, 1, 1] += 1\n",
    "\n",
    "    return A\n",
    "\n",
    "# Compute accuracy.\n",
    "def compute_accuracy(labels, outputs):\n",
    "    # Compute the confusion matrix.\n",
    "    classes = np.unique(np.concatenate((labels, outputs)))\n",
    "    labels = compute_one_hot_encoding(labels, classes)\n",
    "    outputs = compute_one_hot_encoding(outputs, classes)\n",
    "    A = compute_confusion_matrix(labels, outputs, classes)\n",
    "\n",
    "    # Compute accuracy.\n",
    "    if np.sum(A) > 0:\n",
    "        accuracy = np.trace(A) / np.sum(A)\n",
    "    else:\n",
    "        accuracy = float('nan')\n",
    "\n",
    "    # Compute per-class accuracy.\n",
    "    num_classes = len(classes)\n",
    "    per_class_accuracy = np.zeros(num_classes)\n",
    "    for i in range(num_classes):\n",
    "        if np.sum(labels[:, i]) > 0:\n",
    "            per_class_accuracy[i] = A[i, i] / np.sum(A[:, i])\n",
    "        else:\n",
    "            per_class_accuracy[i] = float('nan')\n",
    "\n",
    "    return accuracy, per_class_accuracy, classes\n",
    "\n",
    "# Compute macro F-measure.\n",
    "def compute_f_measure(labels, outputs):\n",
    "    # Compute confusion matrix.\n",
    "    classes = np.unique(np.concatenate((labels, outputs)))\n",
    "    labels = compute_one_hot_encoding(labels, classes)\n",
    "    outputs = compute_one_hot_encoding(outputs, classes)\n",
    "    A = compute_one_vs_rest_confusion_matrix(labels, outputs, classes)\n",
    "\n",
    "    num_classes = len(classes)\n",
    "    per_class_f_measure = np.zeros(num_classes)\n",
    "    for k in range(num_classes):\n",
    "        tp, fp, fn, tn = A[k, 0, 0], A[k, 0, 1], A[k, 1, 0], A[k, 1, 1]\n",
    "        if 2 * tp + fp + fn > 0:\n",
    "            per_class_f_measure[k] = float(2 * tp) / float(2 * tp + fp + fn)\n",
    "        else:\n",
    "            per_class_f_measure[k] = float('nan')\n",
    "\n",
    "    if np.any(np.isfinite(per_class_f_measure)):\n",
    "        macro_f_measure = np.nanmean(per_class_f_measure)\n",
    "    else:\n",
    "        macro_f_measure = float('nan')\n",
    "\n",
    "    return macro_f_measure, per_class_f_measure, classes\n",
    "\n",
    "# Compute mean-squared error.\n",
    "def compute_mse(labels, outputs):\n",
    "    assert len(labels) == len(outputs)\n",
    "\n",
    "    labels = np.asarray(labels, dtype=np.float64)\n",
    "    outputs = np.asarray(outputs, dtype=np.float64)\n",
    "    mse = np.mean((labels - outputs)**2)\n",
    "\n",
    "    return mse\n",
    "\n",
    "# Compute mean-absolute error.\n",
    "def compute_mae(labels, outputs):\n",
    "    assert len(labels) == len(outputs)\n",
    "\n",
    "    labels = np.asarray(labels, dtype=np.float64)\n",
    "    outputs = np.asarray(outputs, dtype=np.float64)\n",
    "    mae = np.mean(np.abs(labels - outputs))\n",
    "\n",
    "    return mae\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Compute the scores for the model outputs.\n",
    "    scores = evaluate_model(\"/Volumes/Bharadwaj/physionet-official/test\", \"/Volumes/Bharadwaj/physionet-official/output\")\n",
    "\n",
    "    # Unpack the scores.\n",
    "    challenge_score, auroc_outcomes, auprc_outcomes, accuracy_outcomes, f_measure_outcomes, mse_cpcs, mae_cpcs = scores\n",
    "\n",
    "    # Construct a string with scores.\n",
    "    output_string = \\\n",
    "        'Challenge Score: {:.3f}\\n'.format(challenge_score) + \\\n",
    "        'Outcome AUROC: {:.3f}\\n'.format(auroc_outcomes) + \\\n",
    "        'Outcome AUPRC: {:.3f}\\n'.format(auprc_outcomes) + \\\n",
    "        'Outcome Accuracy: {:.3f}\\n'.format(accuracy_outcomes) + \\\n",
    "        'Outcome F-measure: {:.3f}\\n'.format(f_measure_outcomes) + \\\n",
    "        'CPC MSE: {:.3f}\\n'.format(mse_cpcs) + \\\n",
    "        'CPC MAE: {:.3f}\\n'.format(mae_cpcs)\n",
    "\n",
    "    print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d5f18a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
